from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Dict
import os
from dotenv import load_dotenv
from openai import OpenAI
from math import prod
from functools import reduce
import math
import re

# Load API key
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

app = FastAPI()

class ChatRequest(BaseModel):
    messages: List[Dict[str, str]]

# GPT Streaming generator
def gpt_stream(messages):
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        stream=True
    )
    for chunk in response:
        if 'choices' in chunk and len(chunk.choices) > 0:
            delta = chunk.choices[0].delta
            if 'content' in delta:
                yield delta.content

# Safe eval ê³„ì‚°
forbidden_keywords = ["import", "def", "exec", "eval", "os.", "__"]

def compute_expression(expr: str) -> str:
    try:
        if any(keyword in expr for keyword in forbidden_keywords):
            return f"ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: ê¸ˆì§€ëœ í‘œí˜„ì‹ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."

        safe_globals = {
            "__builtins__": {},
            "sum": sum,
            "range": range,
            "prod": prod,
            "round": round,
            "reduce": reduce,
            "all": all,
            "int": int,
            "float": float,
            "abs": abs,
            "pow": pow,
            "math": math,
            "sqrt": math.sqrt,
            "log": math.log,
            "log10": math.log10,
            "exp": math.exp
        }
        result = eval(expr, safe_globals)
        return f"ê³„ì‚° ê²°ê³¼: {result}"
    except Exception as e:
        return f"ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

# Improved auto_wrap_latex
def auto_wrap_latex(response: str) -> str:
    # ì´ë¯¸ $$ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë‘”ë‹¤
    if "$$" in response:
        return response

    # LaTeX environments to capture and convert
    environments = [
        "align\\*", "align", "equation", "gather", "multline"
    ]

    # \[ ... \] â†’ $$ ... $$ ë³€í™˜
    response = re.sub(r'\\\[(.*?)\\\]', r'$$\1$$', response, flags=re.DOTALL)

    # \begin{ENV}...\end{ENV} â†’ $$ ... $$ ë³€í™˜
    for env in environments:
        pattern = rf'\\begin{{{env}}}(.*?)\\end{{{env}}}'
        response = re.sub(pattern, r'$$\1$$', response, flags=re.DOTALL)

    # ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ê°ì‹¸ì§€ ì•ŠìŒ (ì„¤ëª… ê°€ëŠ¥ì„± ë†’ìŒ)
    if len(response.strip()) > 300:
        return response

    # ìˆ˜ì‹ keyword heuristic (ì•ˆì „ë§ ìœ ì§€)
    formula_keywords = ["=", "+", "-", "*", "/", "^", "\\approx","\\sqrt", "\\frac", "\\sum", "\\int", "\\log", "\\sin", "\\cos", "\\tan", "\\text", "\\displaystyle"]

    if any(keyword in response for keyword in formula_keywords):
        if "\n" not in response and all(c.isalnum() or c in " +-*/^=()[]{}\\._" for c in response.strip()):
            print("[DEBUG] Auto-wrapping response as LaTeX (fallback keyword match).")
            return f"$$ {response.strip()} $$"

    return response

# âœ… FastAPI @app.post("/chat") ìˆ˜ì •ëœ endpoint ë²„ì „

@app.post("/chat")
async def chat_endpoint(req: ChatRequest):
    messages = req.messages

    user_msgs = [m["content"] for m in messages if m["role"] == "user"]
    last_msg = user_msgs[-1] if user_msgs else ""

    calc_keywords = ["í•©", "ê³±", "í”¼ë³´ë‚˜ì¹˜", "product of primes", "sum of primes", "fibonacci"]

    if any(keyword in last_msg for keyword in calc_keywords):
        # ê³„ì‚° ëª¨ë“œ â†’ ê¸°ì¡´ ë°©ì‹ ìœ ì§€ (stream X)
        system_prompt = [
            {"role": "system", "content": "You are an assistant that converts calculation requests into ONE-LINE Python expressions. "
                                          "You must NOT define functions. You must NOT use 'is_prime' or any undefined functions. "
                                          "You must NOT use import statements. You must NOT use eval or exec or os. "
                                          "You must use list comprehension with 'all(x % d != 0 ...)' inline to detect primes. "
                                          "If the user asks for sum of primes, output 'sum([...])'. "
                                          "If the user asks for product of primes, output 'prod([...])'. "
                                          "If the user asks for the nth Fibonacci number, you MUST use a one-line expression with 'reduce' only. "
                                          "Only output the expression and nothing else."},
            {"role": "user", "content": last_msg}
        ]
        expr = get_chatbot_response(system_prompt)
        print(f"[DEBUG] Generated expression: {repr(expr)}")

        result = compute_expression(expr)
        return {"response": result}

    # ì¼ë°˜ chat ì²˜ë¦¬ â†’ stream íŒŒë¼ë¯¸í„° ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    # req ê°ì²´ì—ëŠ” stream íŒŒë¼ë¯¸í„° ì—†ìŒ â†’ í´ë¼ì´ì–¸íŠ¸ì—ì„œ messages ì™¸ì— stream ì—¬ë¶€ ëª…ì‹œ í•„ìš”
    # ì—¬ê¸°ì„œëŠ” ì„ì‹œë¡œ stream ì—¬ë¶€ë¥¼ query param ìœ¼ë¡œ ë°›ëŠ” ì˜ˆì‹œë¡œ ì‘ì„±
    # ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” ë³„ë„ì˜ /chat_stream endpoint ì¶”ì²œ

    from fastapi import Request
    from fastapi.params import Query

    # stream íŒŒë¼ë¯¸í„° ê¸°ë³¸ False
    import inspect
    frame = inspect.currentframe().f_back
    request = frame.f_locals.get("request", None)

    stream_mode = False
    if request is not None:
        try:
            stream_param = request.query_params.get("stream", "false")
            stream_mode = stream_param.lower() == "true"
        except:
            pass

    system_prompt_default = [
        {"role": "system", "content": "You are a helpful assistant. "
                                      "If your output includes a mathematical formula or expression, surround it with $$...$$ "
                                      "so that it can be rendered as LaTeX. "
                                      "If your output is normal text, do not use $$."},
    ]

    if stream_mode:
        # Streaming mode ì‚¬ìš©
        return StreamingResponse(
            gpt_stream(system_prompt_default + messages),
            media_type="text/plain"
        )
    else:
        # ì¼ë°˜ JSON ì‘ë‹µ
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=system_prompt_default + messages
        )
        return {"response": response.choices[0].message.content.strip()}

# ğŸš€ ì¶”í›„ í™•ì¥ ê°€ëŠ¥: /chat_stream ë³„ë„ endpoint êµ¬ì„± ì¶”ì²œ!


